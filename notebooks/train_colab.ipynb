{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FunctionGemma pour Home Assistant\n",
    "\n",
    "Ce notebook permet d'entraîner **FunctionGemma-270m-it** sur Google Colab.\n",
    "\n",
    "**Pattern multi-turn:**\n",
    "1. User demande une action\n",
    "2. Model appelle `get_entities` pour récupérer les entités du bon domaine\n",
    "3. Tool retourne les entités disponibles\n",
    "4. Model appelle l'action avec la bonne entité\n",
    "\n",
    "**Instructions:**\n",
    "1. Activez le GPU: Runtime → Change runtime type → GPU\n",
    "2. Exécutez les cellules dans l'ordre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation avec versions compatibles\n!pip install -q transformers==4.42.0 peft==0.11.1 accelerate==0.30.0 datasets bitsandbytes huggingface_hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU disponible: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Hugging Face\n",
    "\n",
    "FunctionGemma est un modèle gated:\n",
    "1. Accepter les conditions sur https://huggingface.co/google/functiongemma-270m-it\n",
    "2. Créer un token sur https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"Connecté via secret Colab\")\n",
    "except:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Uploadez train.jsonl et val.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f\"data/{filename}\")\n",
    "    print(f\"  → data/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def count_lines(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "train_count = count_lines(\"data/train.jsonl\")\n",
    "val_count = count_lines(\"data/val.jsonl\")\n",
    "\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Train: {train_count} exemples\")\n",
    "print(f\"  Validation: {val_count} exemples\")\n",
    "\n",
    "# Aperçu\n",
    "with open(\"data/train.jsonl\", 'r') as f:\n",
    "    example = json.loads(f.readline())\n",
    "    print(f\"\\nExemple:\")\n",
    "    print(example['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    \"model_name\": \"google/functiongemma-270m-it\",\n    \"max_length\": 512,  # Plus long pour le multi-turn\n    \n    # LoRA\n    \"lora_r\": 32,  # Augmenté pour meilleure capacité\n    \"lora_alpha\": 64,\n    \"lora_dropout\": 0.05,\n    \"lora_target_modules\": [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    \n    # Entraînement (optimisé pour A100 40GB)\n    \"batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"learning_rate\": 1e-4,\n    \"num_epochs\": 10,\n    \"warmup_ratio\": 0.1,\n    \"weight_decay\": 0.01,\n    \n    \"output_dir\": \"./output\",\n    \"save_steps\": 50,\n    \"logging_steps\": 10,\n}\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nprint(f\"Chargement de {CONFIG['model_name']}...\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    CONFIG[\"model_name\"],\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    CONFIG[\"model_name\"],\n    trust_remote_code=True,\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"Modèle chargé! Paramètres: {model.num_parameters():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Préparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"data/train.jsonl\",\n",
    "        \"validation\": \"data/val.jsonl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "\n",
    "print(\"Tokenization...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    "    batched=True,\n",
    ")\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {CONFIG['num_epochs']} | Batch: {CONFIG['batch_size']} | LR: {CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entraînement...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = f\"{CONFIG['output_dir']}/final\"\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "print(f\"Sauvegardé: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"functiongemma-ha\", 'zip', final_path)\n",
    "files.download(\"functiongemma-ha.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test du modèle\n",
    "\n",
    "**IMPORTANT:** Le format de test doit correspondre EXACTEMENT au format d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_model(query: str):\n    \"\"\"Test le modèle - format IDENTIQUE à l'entraînement.\"\"\"\n    # Format exact du dataset d'entraînement\n    text = f\"<start_of_turn>user\\n{query}<end_of_turn>\\n<start_of_turn>model\\n\"\n    \n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=False,  # Greedy decoding - plus stable\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n    # Extraire la réponse du modèle\n    if \"<start_of_turn>model\" in response:\n        response = response.split(\"<start_of_turn>model\")[-1]\n    if \"<end_of_turn>\" in response:\n        response = response.split(\"<end_of_turn>\")[0]\n    \n    return response.strip()\n\n# Tests avec des requêtes similaires au dataset\ntest_queries = [\n    \"Allume la lumière du salon\",\n    \"Éteins la lumière de la cuisine\",\n    \"Mets le chauffage à 21 degrés\",\n    \"Active le thermostat\",\n    \"Éteins ESPresense\",\n    \"Active la scène cinéma\",\n]\n\nprint(\"Tests du modèle fine-tuné:\\n\")\nfor query in test_queries:\n    print(f\"User: {query}\")\n    response = test_model(query)\n    print(f\"Model: {response}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test multi-turn complet\n",
    "\n",
    "Simuler le flow complet: query → get_entities → tool response → action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_multiturn(query: str, fake_entities: str):\n    \"\"\"Test le flow multi-turn complet.\"\"\"\n    \n    # Étape 1: Requête utilisateur → get_entities\n    text1 = f\"<start_of_turn>user\\n{query}<end_of_turn>\\n<start_of_turn>model\\n\"\n    inputs1 = tokenizer(text1, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        out1 = model.generate(**inputs1, max_new_tokens=80, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    \n    resp1 = tokenizer.decode(out1[0], skip_special_tokens=False)\n    if \"<start_of_turn>model\" in resp1:\n        resp1 = resp1.split(\"<start_of_turn>model\")[-1]\n    if \"<end_of_turn>\" in resp1:\n        resp1 = resp1.split(\"<end_of_turn>\")[0]\n    resp1 = resp1.strip()\n    \n    print(f\"User: {query}\")\n    print(f\"Model (step 1): {resp1}\")\n    \n    # Étape 2: Injecter la réponse tool → action finale\n    text2 = (\n        f\"<start_of_turn>user\\n{query}<end_of_turn>\\n\"\n        f\"<start_of_turn>model\\n{resp1}<end_of_turn>\\n\"\n        f\"<start_of_turn>tool\\n{fake_entities}<end_of_turn>\\n\"\n        f\"<start_of_turn>model\\n\"\n    )\n    inputs2 = tokenizer(text2, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        out2 = model.generate(**inputs2, max_new_tokens=80, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    \n    resp2 = tokenizer.decode(out2[0], skip_special_tokens=False)\n    if \"<start_of_turn>model\" in resp2:\n        resp2 = resp2.split(\"<start_of_turn>model\")[-1]\n    if \"<end_of_turn>\" in resp2:\n        resp2 = resp2.split(\"<end_of_turn>\")[0]\n    resp2 = resp2.strip()\n    \n    print(f\"Tool: {fake_entities[:100]}...\")\n    print(f\"Model (step 2): {resp2}\")\n    print()\n\n# Test\nprint(\"=== Test Multi-Turn ===\")\ntest_multiturn(\n    \"Allume la lumière du salon\",\n    \"Entités light disponibles: light.salon, light.cuisine, light.chambre\"\n)\ntest_multiturn(\n    \"Mets le chauffage à 22 degrés\",\n    \"Entités climate disponibles: climate.thermostat_salon, climate.thermostat_bureau\"\n)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}