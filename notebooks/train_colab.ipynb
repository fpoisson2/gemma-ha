{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning FunctionGemma pour Home Assistant\n\nCe notebook permet d'entraîner **FunctionGemma-270m-it** sur Google Colab avec des optimisations avancées.\n\n**Améliorations:**\n- Métriques personnalisées (précision function calls, entity accuracy)\n- Scheduler cosine avec warmup\n- Early stopping intelligent\n- TensorBoard logging\n- LoRA rank optimisé\n\n**Format one-step (simplifié):**\n1. User envoie une requête + liste des entités disponibles\n2. Model appelle directement l'action avec la bonne entité\n\n**Instructions:**\n1. Activez le GPU: Runtime → Change runtime type → GPU\n2. Exécutez les cellules dans l'ordre"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "!pip install -q transformers peft accelerate datasets bitsandbytes huggingface_hub tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    \n",
    "    # Recommandations automatiques basées sur le GPU\n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"\\n✓ A100 détectée - Configuration optimale disponible\")\n",
    "        RECOMMENDED_BATCH = 16\n",
    "        RECOMMENDED_GRAD_ACCUM = 1\n",
    "    elif \"V100\" in gpu_name or vram_gb >= 16:\n",
    "        print(\"\\n✓ GPU 16GB+ - Bonne configuration disponible\")\n",
    "        RECOMMENDED_BATCH = 8\n",
    "        RECOMMENDED_GRAD_ACCUM = 2\n",
    "    elif \"T4\" in gpu_name or vram_gb >= 12:\n",
    "        print(\"\\n⚠ T4/12GB - Configuration conservative recommandée\")\n",
    "        RECOMMENDED_BATCH = 4\n",
    "        RECOMMENDED_GRAD_ACCUM = 4\n",
    "    else:\n",
    "        print(\"\\n⚠ GPU limitée - Configuration minimale\")\n",
    "        RECOMMENDED_BATCH = 2\n",
    "        RECOMMENDED_GRAD_ACCUM = 8\n",
    "    \n",
    "    print(f\"   Batch recommandé: {RECOMMENDED_BATCH}\")\n",
    "    print(f\"   Gradient accumulation: {RECOMMENDED_GRAD_ACCUM}\")\n",
    "    print(f\"   Effective batch size: {RECOMMENDED_BATCH * RECOMMENDED_GRAD_ACCUM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Hugging Face\n",
    "\n",
    "FunctionGemma est un modèle gated:\n",
    "1. Accepter les conditions sur https://huggingface.co/google/functiongemma-270m-it\n",
    "2. Créer un token sur https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"✓ Connecté via secret Colab\")\n",
    "except:\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Uploadez train.jsonl et val.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f\"data/{filename}\")\n",
    "    print(f\"  → data/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\ndef count_lines(filepath):\n    with open(filepath, 'r') as f:\n        return sum(1 for _ in f)\n\ndef analyze_dataset(filepath):\n    \"\"\"Analyse la distribution du dataset.\"\"\"\n    stats = {\n        \"total\": 0,\n        \"actions\": {},\n        \"negative\": 0,\n    }\n    \n    with open(filepath, 'r') as f:\n        for line in f:\n            stats[\"total\"] += 1\n            example = json.loads(line)\n            text = example.get('text', '')\n            \n            # Détecter les actions\n            for action in ['turn_on', 'turn_off', 'set_temperature', 'set_hvac_mode', \n                          'open_cover', 'close_cover', 'lock', 'unlock', 'activate']:\n                if action in text:\n                    stats[\"actions\"][action] = stats[\"actions\"].get(action, 0) + 1\n            \n            # Exemples négatifs (error.*)\n            if 'error.' in text or 'clarification_needed' in text:\n                stats[\"negative\"] += 1\n    \n    return stats\n\ntrain_count = count_lines(\"data/train.jsonl\")\nval_count = count_lines(\"data/val.jsonl\")\n\nprint(f\"Dataset:\")\nprint(f\"  Train: {train_count} exemples\")\nprint(f\"  Validation: {val_count} exemples\")\nprint(f\"  Ratio val: {val_count/(train_count+val_count)*100:.1f}%\")\n\n# Analyse détaillée\nprint(\"\\nAnalyse du dataset d'entraînement:\")\ntrain_stats = analyze_dataset(\"data/train.jsonl\")\nprint(f\"  Exemples négatifs: {train_stats['negative']} ({train_stats['negative']/train_stats['total']*100:.1f}%)\")\nprint(f\"  Actions:\")\nfor action, count in sorted(train_stats['actions'].items(), key=lambda x: -x[1]):\n    print(f\"    {action}: {count}\")\n\n# Aperçu\nwith open(\"data/train.jsonl\", 'r') as f:\n    example = json.loads(f.readline())\n    print(f\"\\nExemple:\")\n    print(example['text'][:500] + \"...\" if len(example['text']) > 500 else example['text'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "### Hyperparamètres optimisés\n",
    "\n",
    "| Paramètre | Valeur | Justification |\n",
    "|-----------|--------|---------------|\n",
    "| LoRA rank | 64 | Meilleure capacité d'apprentissage |\n",
    "| LoRA alpha | 128 | Ratio alpha/r = 2 (standard) |\n",
    "| Learning rate | 1e-4 | Optimal pour LoRA fine-tuning |\n",
    "| Epochs | 5 | Balance qualité/temps |\n",
    "| Scheduler | Cosine | Meilleure convergence |\n",
    "| Early stopping | 3 | Évite le surapprentissage |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration principale\n",
    "CONFIG = {\n",
    "    \"model_name\": \"google/functiongemma-270m-it\",\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    # LoRA - Augmenté pour meilleure capacité\n",
    "    \"lora_r\": 64,           # Augmenté de 32 à 64\n",
    "    \"lora_alpha\": 128,      # Ratio alpha/r = 2\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    \n",
    "    # Entraînement - Ajuster selon GPU (voir cellule 1)\n",
    "    \"batch_size\": RECOMMENDED_BATCH if 'RECOMMENDED_BATCH' in dir() else 8,\n",
    "    \"gradient_accumulation_steps\": RECOMMENDED_GRAD_ACCUM if 'RECOMMENDED_GRAD_ACCUM' in dir() else 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 5,            # Augmenté pour meilleure convergence\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Scheduler\n",
    "    \"lr_scheduler_type\": \"cosine\",  # Nouveau: scheduler cosine\n",
    "    \n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 3,   # Nouveau: arrêt après 3 eval sans amélioration\n",
    "    \"early_stopping_threshold\": 0.01,\n",
    "    \n",
    "    # Sauvegarde\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"save_steps\": 50,\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 50,\n",
    "}\n",
    "\n",
    "# Calcul de l'effective batch size\n",
    "effective_batch = CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  LoRA rank: {CONFIG['lora_r']} (alpha: {CONFIG['lora_alpha']})\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']} × {CONFIG['gradient_accumulation_steps']} = {effective_batch} effective\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']} scheduler)\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Early stopping: patience={CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"Chargement de {CONFIG['model_name']}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Modèle chargé! Paramètres: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration LoRA optimisée\nlora_config = LoraConfig(\n    r=CONFIG[\"lora_r\"],\n    lora_alpha=CONFIG[\"lora_alpha\"],\n    lora_dropout=CONFIG[\"lora_dropout\"],\n    target_modules=CONFIG[\"lora_target_modules\"],\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# IMPORTANT: Activer input_require_grads AVANT gradient checkpointing\n# Cela est nécessaire pour PEFT/LoRA avec gradient checkpointing\nmodel.enable_input_require_grads()\n\n# Activer gradient checkpointing pour économiser la mémoire\n# use_reentrant=False est requis pour PyTorch 2.x avec PEFT\nmodel.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\nprint(\"✓ Gradient checkpointing activé (use_reentrant=False)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Préparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"data/train.jsonl\",\n",
    "        \"validation\": \"data/val.jsonl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "    # Pour le causal LM, les labels sont les mêmes que input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenization...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"],\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\"✓ Tokenization terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Métriques personnalisées\n",
    "\n",
    "Évaluation spécifique aux function calls:\n",
    "- **Function Call Accuracy**: Le modèle appelle-t-il la bonne fonction?\n",
    "- **Entity Accuracy**: Le modèle sélectionne-t-il la bonne entité?\n",
    "- **Negative Detection**: Le modèle détecte-t-il les requêtes impossibles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport numpy as np\nfrom transformers import EvalPrediction\n\ndef extract_function_call(text):\n    \"\"\"Extrait le nom de fonction et les paramètres d'un appel FunctionGemma.\n    \n    Format attendu: <start_function_call>call:func_name{param:value,...}<end_function_call>\n    \"\"\"\n    # Pattern pour le format FunctionGemma\n    match = re.search(r'call:([a-z_\\.]+)\\{([^}]*)\\}', text)\n    if match:\n        func_name = match.group(1)\n        params_str = match.group(2)\n        \n        # Extraire entity_id si présent\n        # Format: entity_id:<escape>value<escape> ou entity_id:value\n        entity_match = re.search(r'entity_id:(?:<escape>)?([^<,]+)(?:<escape>)?', params_str)\n        entity_id = entity_match.group(1).strip() if entity_match else None\n        \n        return func_name, entity_id\n    return None, None\n\ndef compute_metrics(eval_pred: EvalPrediction):\n    \"\"\"Calcule les métriques personnalisées.\"\"\"\n    predictions, labels = eval_pred\n    \n    # Décoder les prédictions\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    # Pour la perplexité, on calcule la loss moyenne\n    # Note: Les métriques de function call nécessitent une génération complète\n    # qui est faite séparément dans l'évaluation détaillée\n    \n    # Calculer la perplexité à partir des logits\n    shift_logits = predictions[..., :-1, :]\n    shift_labels = labels[..., 1:]\n    \n    # Masquer les tokens de padding (-100)\n    mask = shift_labels != -100\n    \n    if mask.sum() > 0:\n        # Calculer la cross-entropy\n        from torch.nn import CrossEntropyLoss\n        loss_fct = CrossEntropyLoss(reduction='none')\n        \n        flat_logits = torch.tensor(shift_logits).view(-1, shift_logits.shape[-1])\n        flat_labels = torch.tensor(shift_labels).view(-1)\n        \n        losses = loss_fct(flat_logits, flat_labels)\n        masked_losses = losses * mask.view(-1).float()\n        \n        avg_loss = masked_losses.sum() / mask.sum()\n        perplexity = torch.exp(avg_loss).item()\n    else:\n        perplexity = float('inf')\n    \n    return {\n        \"perplexity\": perplexity,\n    }\n\nprint(\"✓ Métriques personnalisées définies\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import (\n    TrainingArguments, \n    Trainer, \n    DataCollatorForLanguageModeling,\n    EarlyStoppingCallback,\n)\n\ntraining_args = TrainingArguments(\n    output_dir=CONFIG[\"output_dir\"],\n    \n    # Epochs et batch\n    num_train_epochs=CONFIG[\"num_epochs\"],\n    per_device_train_batch_size=CONFIG[\"batch_size\"],\n    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n    \n    # Optimisation\n    learning_rate=CONFIG[\"learning_rate\"],\n    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],  # Cosine scheduler\n    warmup_ratio=CONFIG[\"warmup_ratio\"],\n    weight_decay=CONFIG[\"weight_decay\"],\n    max_grad_norm=1.0,\n    \n    # Logging\n    logging_dir=\"./logs\",\n    logging_steps=CONFIG[\"logging_steps\"],\n    report_to=[\"tensorboard\"],  # Activer TensorBoard\n    \n    # Évaluation\n    eval_strategy=\"steps\",\n    eval_steps=CONFIG[\"eval_steps\"],\n    \n    # Sauvegarde\n    save_strategy=\"steps\",\n    save_steps=CONFIG[\"save_steps\"],\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    # Performance\n    bf16=True,\n    dataloader_num_workers=2,\n    # Gradient checkpointing déjà activé manuellement dans la cellule précédente\n    # avec use_reentrant=False pour compatibilité PEFT/LoRA\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    # Misc\n    remove_unused_columns=False,\n    seed=42,\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=False,\n)\n\n# Callbacks\ncallbacks = [\n    EarlyStoppingCallback(\n        early_stopping_patience=CONFIG[\"early_stopping_patience\"],\n        early_stopping_threshold=CONFIG[\"early_stopping_threshold\"],\n    )\n]\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n    callbacks=callbacks,\n)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Configuration d'entraînement:\")\nprint(f\"  Epochs: {CONFIG['num_epochs']}\")\nprint(f\"  Batch: {CONFIG['batch_size']} × {CONFIG['gradient_accumulation_steps']} = {effective_batch}\")\nprint(f\"  LR: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']})\")\nprint(f\"  Early stopping: patience={CONFIG['early_stopping_patience']}\")\nprint(f\"  TensorBoard: ./logs\")\nprint(f\"  Gradient checkpointing: use_reentrant=False\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer TensorBoard (optionnel)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Début de l'entraînement...\")\n",
    "print(f\"  Train: {len(tokenized_dataset['train'])} exemples\")\n",
    "print(f\"  Val: {len(tokenized_dataset['validation'])} exemples\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Entraînement terminé!\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Steps: {train_result.global_step}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation finale\n",
    "print(\"Évaluation finale...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nRésultats:\")\n",
    "print(f\"  Eval loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"  Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Évaluation détaillée des Function Calls\n",
    "\n",
    "Test de la qualité des prédictions sur des exemples spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nfrom collections import defaultdict\n\n# Entités de test par domaine (simuler ce qui serait dans le prompt)\n# IMPORTANT: Le modèle est entraîné avec TOUTES les entités de TOUS les domaines\nTEST_ENTITIES = {\n    \"light\": [\"light.salon\", \"light.cuisine\", \"light.chambre\", \"light.bureau\"],\n    \"switch\": [\"switch.prise_salon\", \"switch.prise_cuisine\"],\n    \"climate\": [\"climate.thermostat_salon\", \"climate.thermostat_bureau\"],\n    \"scene\": [\"scene.cinema\", \"scene.nuit\", \"scene.romantique\"],\n    \"cover\": [\"cover.volets_salon\", \"cover.volets_chambre\", \"cover.volets_cuisine\"],\n    \"fan\": [\"fan.ventilateur_salon\", \"fan.ventilateur_chambre\"],\n    \"lock\": [\"lock.porte_entree\", \"lock.porte_garage\"],\n    \"person\": [\"person.francis\", \"person.noemie\"],\n}\n\n# Ordre des domaines (identique à dataset_generator.py)\nDOMAIN_ORDER = ['light', 'switch', 'climate', 'scene', 'cover', 'fan', 'lock', 'person']\n\ndef build_all_entities_context():\n    \"\"\"Construit le contexte avec TOUTES les entités (comme en production).\"\"\"\n    parts = []\n    for domain in DOMAIN_ORDER:\n        entities = TEST_ENTITIES.get(domain, [])\n        if entities:\n            entities_str = \", \".join(entities)\n            parts.append(f\"Entités {domain} disponibles: {entities_str}\")\n    return \"\\n\".join(parts)\n\ndef generate_response_with_entities(query: str, max_tokens: int = 100):\n    \"\"\"Génère une réponse avec TOUTES les entités dans le prompt (format one-step).\"\"\"\n    # Utiliser le contexte complet avec toutes les entités\n    entities_context = build_all_entities_context()\n    \n    text = (\n        f\"<start_of_turn>user\\n{query}\\n\\n\"\n        f\"{entities_context}<end_of_turn>\\n\"\n        f\"<start_of_turn>model\\n\"\n    )\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n    if \"<start_of_turn>model\" in response:\n        response = response.split(\"<start_of_turn>model\")[-1]\n    if \"<end_of_turn>\" in response:\n        response = response.split(\"<end_of_turn>\")[0]\n    \n    return response.strip()\n\ndef parse_function_call_full(response: str) -> dict:\n    \"\"\"Parse un appel de fonction FunctionGemma.\"\"\"\n    match = re.search(r'call:([a-z_\\.]+)\\{([^}]*)\\}', response)\n    if not match:\n        return None\n    \n    func_name = match.group(1)\n    params_str = match.group(2)\n    \n    params = {}\n    for param in params_str.split(','):\n        if ':' in param:\n            key, value = param.split(':', 1)\n            value = value.replace('<escape>', '').strip()\n            params[key.strip()] = value\n    \n    return {\"name\": func_name, \"params\": params}\n\ndef evaluate_function_calls(test_cases):\n    \"\"\"Évalue la précision des function calls (format one-step).\"\"\"\n    results = {\n        \"total\": 0,\n        \"correct_function\": 0,\n        \"correct_entity\": 0,\n        \"details\": []\n    }\n    \n    for test in test_cases:\n        query = test[\"query\"]\n        expected_func = test[\"expected_function\"]\n        expected_entity = test.get(\"expected_entity\")\n        \n        response = generate_response_with_entities(query)\n        parsed = parse_function_call_full(response)\n        \n        func_name = parsed[\"name\"] if parsed else None\n        params = parsed[\"params\"] if parsed else {}\n        entity_id = params.get(\"entity_id\")\n        \n        results[\"total\"] += 1\n        \n        # Vérifier la fonction\n        if func_name == expected_func:\n            results[\"correct_function\"] += 1\n        \n        # Vérifier l'entité\n        if expected_entity and entity_id == expected_entity:\n            results[\"correct_entity\"] += 1\n        \n        results[\"details\"].append({\n            \"query\": query,\n            \"response\": response,\n            \"function\": func_name,\n            \"entity\": entity_id,\n            \"expected_func\": expected_func,\n            \"expected_entity\": expected_entity,\n        })\n    \n    return results\n\n# Cas de test - format one-step (action directe avec TOUTES les entités)\ntest_cases = [\n    {\"query\": \"Allume la lumière du salon\", \"expected_function\": \"light.turn_on\", \"expected_entity\": \"light.salon\"},\n    {\"query\": \"Éteins la lumière de la cuisine\", \"expected_function\": \"light.turn_off\", \"expected_entity\": \"light.cuisine\"},\n    {\"query\": \"Mets le chauffage à 21 degrés\", \"expected_function\": \"climate.set_temperature\", \"expected_entity\": \"climate.thermostat_salon\"},\n    {\"query\": \"Ferme les volets du salon\", \"expected_function\": \"cover.close_cover\", \"expected_entity\": \"cover.volets_salon\"},\n    {\"query\": \"Active la scène cinéma\", \"expected_function\": \"scene.turn_on\", \"expected_entity\": \"scene.cinema\"},\n    {\"query\": \"Verrouille la porte d'entrée\", \"expected_function\": \"lock.lock\", \"expected_entity\": \"lock.porte_entree\"},\n    {\"query\": \"Allume le ventilateur du salon\", \"expected_function\": \"fan.turn_on\", \"expected_entity\": \"fan.ventilateur_salon\"},\n]\n\nprint(\"Évaluation des function calls (format one-step)...\\n\")\nprint(\"Contexte utilisé (toutes les entités):\")\nprint(\"-\" * 40)\nprint(build_all_entities_context())\nprint(\"-\" * 40 + \"\\n\")\n\nresults = evaluate_function_calls(test_cases)\n\nprint(f\"Résultats ({results['total']} tests):\")\nprint(f\"  Fonction correcte: {results['correct_function']}/{results['total']} ({results['correct_function']/results['total']*100:.1f}%)\")\nprint(f\"  Entité correcte: {results['correct_entity']}/{results['total']} ({results['correct_entity']/results['total']*100:.1f}%)\")\n\nprint(\"\\nDétails:\")\nfor detail in results[\"details\"]:\n    func_ok = \"✓\" if detail[\"function\"] == detail[\"expected_func\"] else \"✗\"\n    entity_ok = \"✓\" if detail[\"entity\"] == detail[\"expected_entity\"] else \"✗\"\n    print(f\"  {func_ok}{entity_ok} {detail['query'][:35]}...\")\n    print(f\"       → {detail['function']}(entity_id={detail['entity']})\")\n    if detail[\"function\"] != detail[\"expected_func\"]:\n        print(f\"       ⚠ Attendu: {detail['expected_func']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = f\"{CONFIG['output_dir']}/final\"\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "import json\n",
    "metrics = {\n",
    "    \"train_loss\": train_result.training_loss,\n",
    "    \"eval_loss\": eval_results['eval_loss'],\n",
    "    \"perplexity\": float(np.exp(eval_results['eval_loss'])),\n",
    "    \"config\": CONFIG,\n",
    "    \"function_call_accuracy\": results['correct_function'] / results['total'] if results['total'] > 0 else 0,\n",
    "}\n",
    "\n",
    "with open(f\"{final_path}/training_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"✓ Modèle sauvegardé: {final_path}\")\n",
    "print(f\"✓ Métriques sauvegardées: {final_path}/training_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"functiongemma-ha\", 'zip', final_path)\n",
    "files.download(\"functiongemma-ha.zip\")\n",
    "print(\"✓ Téléchargement du modèle...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test du modèle\n",
    "\n",
    "**IMPORTANT:** Le format de test doit correspondre EXACTEMENT au format d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_model_onestep(query: str):\n    \"\"\"Test le modèle avec format one-step (TOUTES les entités dans le prompt).\"\"\"\n    response = generate_response_with_entities(query)\n    return response\n\n# Tests avec différents domaines\ntest_queries = [\n    \"Allume la lumière du salon\",\n    \"Éteins la lumière de la cuisine\",\n    \"Mets le chauffage à 21 degrés\",\n    \"Ferme les volets de la chambre\",\n    \"Active la scène cinéma\",\n    \"Verrouille la porte d'entrée\",\n    # Avec typos\n    \"alume la lumiere du salon\",\n    \"etein la cuisine\",\n    # Québécois\n    \"Ferme la lumière du bureau\",\n    \"Ouvre les lumières de la chambre\",\n]\n\nprint(\"Tests du modèle fine-tuné (format one-step):\\n\")\nprint(\"Contexte: toutes les entités sont dans le prompt\")\nprint(\"=\" * 50 + \"\\n\")\n\nfor query in test_queries:\n    print(f\"User: {query}\")\n    response = test_model_onestep(query)\n    parsed = parse_function_call_full(response)\n    \n    if parsed:\n        func = parsed[\"name\"]\n        entity = parsed[\"params\"].get(\"entity_id\", \"?\")\n        print(f\"Model: {func}({entity})\")\n    else:\n        print(f\"Model: {response[:80]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Tests supplémentaires\n\nTests avec différentes variations et cas limites."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tests de robustesse\nprint(\"=\" * 50)\nprint(\"Tests de robustesse\")\nprint(\"=\" * 50 + \"\\n\")\n\nrobustness_tests = [\n    # Variations de formulation\n    \"Peux-tu allumer la lumière du salon ?\",\n    \"Je voudrais éteindre la cuisine\",\n    \"Mets-moi 22 degrés stp\",\n    \n    # Québécois\n    \"Ferme la lumière du salon\",\n    \"Ouvre les lumières de la chambre\",\n    \n    # Fautes de frappe\n    \"alume le slaon\",\n    \"etein tou\",\n    \n    # Formulations naturelles\n    \"Il fait trop froid\",\n    \"J'ai besoin de lumière dans le bureau\",\n    \"Cache le soleil dans la chambre\",\n    \n    # Scènes\n    \"Mets l'ambiance cinéma\",\n    \"Mode nuit\",\n    \n    # Personnes\n    \"Où est Francis ?\",\n    \"Noémie est à la maison ?\",\n]\n\nfor query in robustness_tests:\n    response = generate_response_with_entities(query)\n    parsed = parse_function_call_full(response)\n    \n    if parsed:\n        func = parsed[\"name\"]\n        entity = parsed[\"params\"].get(\"entity_id\", \"?\")\n        print(f\"✓ {query[:40]}\")\n        print(f\"  → {func}({entity})\")\n    else:\n        print(f\"✗ {query[:40]}\")\n        print(f\"  → Pas de function call: {response[:50]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Résumé et prochaines étapes\n",
    "\n",
    "### Métriques finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"RÉSUMÉ DE L'ENTRAÎNEMENT\")\nprint(\"=\"*60)\nprint(f\"\\nModèle: {CONFIG['model_name']}\")\nprint(f\"LoRA rank: {CONFIG['lora_r']} (alpha: {CONFIG['lora_alpha']})\")\nprint(f\"\\nFormat: One-step (entités dans le prompt)\")\nprint(f\"\\nDataset:\")\nprint(f\"  Train: {len(tokenized_dataset['train'])} exemples\")\nprint(f\"  Validation: {len(tokenized_dataset['validation'])} exemples\")\nprint(f\"\\nEntraînement:\")\nprint(f\"  Epochs: {CONFIG['num_epochs']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']})\")\nprint(f\"  Batch size: {effective_batch} (effective)\")\nprint(f\"\\nRésultats:\")\nprint(f\"  Training loss: {train_result.training_loss:.4f}\")\nprint(f\"  Eval loss: {eval_results['eval_loss']:.4f}\")\nprint(f\"  Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\nprint(f\"  Function accuracy: {results['correct_function']/results['total']*100:.1f}%\")\nprint(f\"  Entity accuracy: {results['correct_entity']/results['total']*100:.1f}%\")\nprint(f\"\\nFichiers:\")\nprint(f\"  Modèle: {final_path}/\")\nprint(f\"  Métriques: {final_path}/training_metrics.json\")\nprint(f\"  Logs: ./logs/\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}