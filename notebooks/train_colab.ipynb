{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FunctionGemma pour Home Assistant\n",
    "\n",
    "Ce notebook permet d'entraîner FunctionGemma sur Google Colab avec un GPU gratuit.\n",
    "\n",
    "**Prérequis:**\n",
    "- Un compte Hugging Face avec accès à FunctionGemma\n",
    "- Vos fichiers `train.jsonl` et `val.jsonl` générés localement\n",
    "\n",
    "**Instructions:**\n",
    "1. Activez le GPU: Runtime → Change runtime type → T4 GPU (ou A100 avec Colab Pro)\n",
    "2. Exécutez les cellules dans l'ordre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.40.0 datasets accelerate peft bitsandbytes huggingface_hub trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le GPU disponible\n",
    "import torch\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Hugging Face\n",
    "\n",
    "Créez un token ici: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Token stocké dans les secrets Colab (recommandé)\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"Connecté via secret Colab\")\n",
    "except:\n",
    "    # Option 2: Saisie manuelle\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload du dataset\n",
    "\n",
    "Uploadez vos fichiers `train.jsonl` et `val.jsonl` générés avec `python scripts/generate_dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Créer le dossier data\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Uploadez train.jsonl et val.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Déplacer vers le dossier data\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f\"data/{filename}\")\n",
    "    print(f\"  → data/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le dataset\n",
    "import json\n",
    "\n",
    "def count_lines(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "train_count = count_lines(\"data/train.jsonl\")\n",
    "val_count = count_lines(\"data/val.jsonl\")\n",
    "\n",
    "print(f\"Dataset chargé:\")\n",
    "print(f\"  Train: {train_count} exemples\")\n",
    "print(f\"  Validation: {val_count} exemples\")\n",
    "\n",
    "# Aperçu d'un exemple\n",
    "with open(\"data/train.jsonl\", 'r') as f:\n",
    "    example = json.loads(f.readline())\n",
    "    print(f\"\\nExemple:\")\n",
    "    for msg in example['messages']:\n",
    "        print(f\"  [{msg['role']}]: {msg['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entraînement\n",
    "CONFIG = {\n",
    "    # Modèle\n",
    "    \"model_name\": \"google/functiongemma-270m-it\",\n",
    "    \"max_length\": 1024,  # Réduit pour économiser la mémoire sur T4\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    \n",
    "    # Entraînement - Ajusté pour Colab T4 (16GB VRAM)\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,  # Effective batch = 16\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Sauvegarde\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"Chargement de {CONFIG['model_name']}...\")\n",
    "\n",
    "# Quantization 4-bit pour économiser la mémoire\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Charger le modèle\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Modèle chargé!\")\n",
    "print(f\"  Paramètres: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurer LoRA\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Préparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"data/train.jsonl\",\n",
    "        \"validation\": \"data/val.jsonl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Train: {len(dataset['train'])} exemples\")\n",
    "print(f\"  Validation: {len(dataset['validation'])} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Formate un exemple pour FunctionGemma.\"\"\"\n",
    "    messages = example[\"messages\"]\n",
    "    tools = example.get(\"tools\", [])\n",
    "    \n",
    "    # Essayer d'utiliser le chat template natif\n",
    "    try:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "    except:\n",
    "        # Fallback: format manuel\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"developer\":\n",
    "                text += f\"<start_of_turn>developer\\n{content}<end_of_turn>\\n\"\n",
    "            elif role == \"user\":\n",
    "                text += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                text += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize les exemples.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "    )\n",
    "\n",
    "# Préparation\n",
    "print(\"Formatage du dataset...\")\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "print(\"Tokenization...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(\"Dataset prêt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"paged_adamw_8bit\",  # Optimiseur économe en mémoire\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer configuré!\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer l'entraînement!\n",
    "print(\"Démarrage de l'entraînement...\")\n",
    "print(\"(Cela peut prendre 30min à 2h selon la taille du dataset)\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle fine-tuné\n",
    "final_path = f\"{CONFIG['output_dir']}/final\"\n",
    "\n",
    "print(f\"Sauvegarde vers {final_path}...\")\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "\n",
    "print(\"Modèle sauvegardé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une archive ZIP pour téléchargement\n",
    "import shutil\n",
    "\n",
    "zip_path = \"functiongemma-ha-finetuned\"\n",
    "shutil.make_archive(zip_path, 'zip', final_path)\n",
    "\n",
    "print(f\"Archive créée: {zip_path}.zip\")\n",
    "\n",
    "# Télécharger\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test du modèle (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le modèle fine-tuné\n",
    "def test_model(query: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Tu es un assistant qui contrôle une maison intelligente avec Home Assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Formater l'input\n",
    "    try:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    except:\n",
    "        text = f\"<start_of_turn>developer\\n{messages[0]['content']}<end_of_turn>\\n\"\n",
    "        text += f\"<start_of_turn>user\\n{messages[1]['content']}<end_of_turn>\\n\"\n",
    "        text += \"<start_of_turn>model\\n\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extraire seulement la réponse du modèle\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        response = response.split(\"<start_of_turn>model\")[-1]\n",
    "    if \"<end_of_turn>\" in response:\n",
    "        response = response.split(\"<end_of_turn>\")[0]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Tests\n",
    "test_queries = [\n",
    "    \"Allume la lumière du salon\",\n",
    "    \"Mets le chauffage à 21 degrés\",\n",
    "    \"Ferme les volets de la chambre\",\n",
    "]\n",
    "\n",
    "print(\"Tests du modèle fine-tuné:\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"User: {query}\")\n",
    "    response = test_model(query)\n",
    "    print(f\"Model: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Upload vers Hugging Face Hub (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décommenter pour upload vers HuggingFace Hub\n",
    "# REPO_NAME = \"votre-username/functiongemma-ha\"  # Changez ceci!\n",
    "\n",
    "# model.push_to_hub(REPO_NAME, private=True)\n",
    "# tokenizer.push_to_hub(REPO_NAME, private=True)\n",
    "# print(f\"Modèle uploadé vers: https://huggingface.co/{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Utilisation après téléchargement\n",
    "\n",
    "Pour utiliser le modèle sur votre machine locale:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Charger le modèle de base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/functiongemma-270m-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/functiongemma-270m-it\")\n",
    "\n",
    "# Charger les poids LoRA\n",
    "model = PeftModel.from_pretrained(base_model, \"./output/final\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
