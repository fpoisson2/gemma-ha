{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning FunctionGemma pour Home Assistant\n",
        "\n",
        "Ce notebook permet d'entraîner **FunctionGemma-270m-it** sur Google Colab avec des optimisations avancées.\n",
        "\n",
        "**Améliorations:**\n",
        "- Métriques personnalisées (précision function calls, entity accuracy)\n",
        "- Scheduler cosine avec warmup\n",
        "- Early stopping intelligent\n",
        "- TensorBoard logging\n",
        "- LoRA rank optimisé\n",
        "\n",
        "**Pattern multi-turn:**\n",
        "1. User demande une action\n",
        "2. Model appelle `get_entities` pour récupérer les entités du bon domaine\n",
        "3. Tool retourne les entités disponibles\n",
        "4. Model appelle l'action avec la bonne entité\n",
        "\n",
        "**Instructions:**\n",
        "1. Activez le GPU: Runtime → Change runtime type → GPU\n",
        "2. Exécutez les cellules dans l'ordre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation des dépendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des dépendances\n",
        "!pip install -q transformers peft accelerate datasets bitsandbytes huggingface_hub tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
        "    \n",
        "    # Recommandations automatiques basées sur le GPU\n",
        "    if \"A100\" in gpu_name:\n",
        "        print(\"\\n✓ A100 détectée - Configuration optimale disponible\")\n",
        "        RECOMMENDED_BATCH = 16\n",
        "        RECOMMENDED_GRAD_ACCUM = 1\n",
        "    elif \"V100\" in gpu_name or vram_gb >= 16:\n",
        "        print(\"\\n✓ GPU 16GB+ - Bonne configuration disponible\")\n",
        "        RECOMMENDED_BATCH = 8\n",
        "        RECOMMENDED_GRAD_ACCUM = 2\n",
        "    elif \"T4\" in gpu_name or vram_gb >= 12:\n",
        "        print(\"\\n⚠ T4/12GB - Configuration conservative recommandée\")\n",
        "        RECOMMENDED_BATCH = 4\n",
        "        RECOMMENDED_GRAD_ACCUM = 4\n",
        "    else:\n",
        "        print(\"\\n⚠ GPU limitée - Configuration minimale\")\n",
        "        RECOMMENDED_BATCH = 2\n",
        "        RECOMMENDED_GRAD_ACCUM = 8\n",
        "    \n",
        "    print(f\"   Batch recommandé: {RECOMMENDED_BATCH}\")\n",
        "    print(f\"   Gradient accumulation: {RECOMMENDED_GRAD_ACCUM}\")\n",
        "    print(f\"   Effective batch size: {RECOMMENDED_BATCH * RECOMMENDED_GRAD_ACCUM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Hugging Face\n",
        "\n",
        "FunctionGemma est un modèle gated:\n",
        "1. Accepter les conditions sur https://huggingface.co/google/functiongemma-270m-it\n",
        "2. Créer un token sur https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "    print(\"✓ Connecté via secret Colab\")\n",
        "except:\n",
        "    login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "print(\"Uploadez train.jsonl et val.jsonl\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f\"data/{filename}\")\n",
        "    print(f\"  → data/{filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def count_lines(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "def analyze_dataset(filepath):\n",
        "    \"\"\"Analyse la distribution du dataset.\"\"\"\n",
        "    stats = {\n",
        "        \"total\": 0,\n",
        "        \"get_entities\": 0,\n",
        "        \"actions\": {},\n",
        "        \"negative\": 0,\n",
        "    }\n",
        "    \n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            stats[\"total\"] += 1\n",
        "            example = json.loads(line)\n",
        "            text = example.get('text', '')\n",
        "            \n",
        "            if 'get_entities' in text:\n",
        "                stats[\"get_entities\"] += 1\n",
        "            \n",
        "            # Détecter les actions\n",
        "            for action in ['turn_on', 'turn_off', 'set_temperature', 'set_hvac_mode', \n",
        "                          'open_cover', 'close_cover', 'lock', 'unlock', 'activate']:\n",
        "                if action in text:\n",
        "                    stats[\"actions\"][action] = stats[\"actions\"].get(action, 0) + 1\n",
        "            \n",
        "            # Exemples négatifs\n",
        "            if 'pas trouvé' in text or 'impossible' in text.lower() or 'préciser' in text:\n",
        "                stats[\"negative\"] += 1\n",
        "    \n",
        "    return stats\n",
        "\n",
        "train_count = count_lines(\"data/train.jsonl\")\n",
        "val_count = count_lines(\"data/val.jsonl\")\n",
        "\n",
        "print(f\"Dataset:\")\n",
        "print(f\"  Train: {train_count} exemples\")\n",
        "print(f\"  Validation: {val_count} exemples\")\n",
        "print(f\"  Ratio val: {val_count/(train_count+val_count)*100:.1f}%\")\n",
        "\n",
        "# Analyse détaillée\n",
        "print(\"\\nAnalyse du dataset d'entraînement:\")\n",
        "train_stats = analyze_dataset(\"data/train.jsonl\")\n",
        "print(f\"  Exemples avec get_entities: {train_stats['get_entities']} ({train_stats['get_entities']/train_stats['total']*100:.1f}%)\")\n",
        "print(f\"  Exemples négatifs: {train_stats['negative']} ({train_stats['negative']/train_stats['total']*100:.1f}%)\")\n",
        "print(f\"  Actions:\")\n",
        "for action, count in sorted(train_stats['actions'].items(), key=lambda x: -x[1]):\n",
        "    print(f\"    {action}: {count}\")\n",
        "\n",
        "# Aperçu\n",
        "with open(\"data/train.jsonl\", 'r') as f:\n",
        "    example = json.loads(f.readline())\n",
        "    print(f\"\\nExemple:\")\n",
        "    print(example['text'][:500] + \"...\" if len(example['text']) > 500 else example['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration\n",
        "\n",
        "### Hyperparamètres optimisés\n",
        "\n",
        "| Paramètre | Valeur | Justification |\n",
        "|-----------|--------|---------------|\n",
        "| LoRA rank | 64 | Meilleure capacité d'apprentissage |\n",
        "| LoRA alpha | 128 | Ratio alpha/r = 2 (standard) |\n",
        "| Learning rate | 1e-4 | Optimal pour LoRA fine-tuning |\n",
        "| Epochs | 5 | Balance qualité/temps |\n",
        "| Scheduler | Cosine | Meilleure convergence |\n",
        "| Early stopping | 3 | Évite le surapprentissage |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration principale\n",
        "CONFIG = {\n",
        "    \"model_name\": \"google/functiongemma-270m-it\",\n",
        "    \"max_length\": 512,\n",
        "    \n",
        "    # LoRA - Augmenté pour meilleure capacité\n",
        "    \"lora_r\": 64,           # Augmenté de 32 à 64\n",
        "    \"lora_alpha\": 128,      # Ratio alpha/r = 2\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"lora_target_modules\": [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    \n",
        "    # Entraînement - Ajuster selon GPU (voir cellule 1)\n",
        "    \"batch_size\": RECOMMENDED_BATCH if 'RECOMMENDED_BATCH' in dir() else 8,\n",
        "    \"gradient_accumulation_steps\": RECOMMENDED_GRAD_ACCUM if 'RECOMMENDED_GRAD_ACCUM' in dir() else 2,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_epochs\": 5,            # Augmenté pour meilleure convergence\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \n",
        "    # Scheduler\n",
        "    \"lr_scheduler_type\": \"cosine\",  # Nouveau: scheduler cosine\n",
        "    \n",
        "    # Early stopping\n",
        "    \"early_stopping_patience\": 3,   # Nouveau: arrêt après 3 eval sans amélioration\n",
        "    \"early_stopping_threshold\": 0.01,\n",
        "    \n",
        "    # Sauvegarde\n",
        "    \"output_dir\": \"./output\",\n",
        "    \"save_steps\": 50,\n",
        "    \"logging_steps\": 10,\n",
        "    \"eval_steps\": 50,\n",
        "}\n",
        "\n",
        "# Calcul de l'effective batch size\n",
        "effective_batch = CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation_steps\"]\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {CONFIG['model_name']}\")\n",
        "print(f\"  LoRA rank: {CONFIG['lora_r']} (alpha: {CONFIG['lora_alpha']})\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']} × {CONFIG['gradient_accumulation_steps']} = {effective_batch} effective\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']} scheduler)\")\n",
        "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"  Early stopping: patience={CONFIG['early_stopping_patience']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chargement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "print(f\"Chargement de {CONFIG['model_name']}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CONFIG[\"model_name\"],\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"✓ Modèle chargé! Paramètres: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration LoRA optimisée\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG[\"lora_r\"],\n",
        "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
        "    target_modules=CONFIG[\"lora_target_modules\"],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Activer gradient checkpointing pour économiser la mémoire\n",
        "model.gradient_checkpointing_enable()\n",
        "print(\"✓ Gradient checkpointing activé\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Préparation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        \"train\": \"data/train.jsonl\",\n",
        "        \"validation\": \"data/val.jsonl\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=CONFIG[\"max_length\"],\n",
        "    )\n",
        "    # Pour le causal LM, les labels sont les mêmes que input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenization...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=[\"text\"],\n",
        "    batched=True,\n",
        "    desc=\"Tokenizing\",\n",
        ")\n",
        "print(\"✓ Tokenization terminée\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Métriques personnalisées\n",
        "\n",
        "Évaluation spécifique aux function calls:\n",
        "- **Function Call Accuracy**: Le modèle appelle-t-il la bonne fonction?\n",
        "- **Entity Accuracy**: Le modèle sélectionne-t-il la bonne entité?\n",
        "- **Negative Detection**: Le modèle détecte-t-il les requêtes impossibles?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "def extract_function_call(text):\n",
        "    \"\"\"Extrait le nom de fonction et les paramètres d'un appel.\"\"\"\n",
        "    # Pattern: function_name(param1=\"value1\", ...)\n",
        "    match = re.search(r'(\\w+)\\s*\\(([^)]*)\\)', text)\n",
        "    if match:\n",
        "        func_name = match.group(1)\n",
        "        params_str = match.group(2)\n",
        "        \n",
        "        # Extraire entity_id si présent\n",
        "        entity_match = re.search(r'entity_id\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', params_str)\n",
        "        entity_id = entity_match.group(1) if entity_match else None\n",
        "        \n",
        "        return func_name, entity_id\n",
        "    return None, None\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"Calcule les métriques personnalisées.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # Décoder les prédictions\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    \n",
        "    # Pour la perplexité, on calcule la loss moyenne\n",
        "    # Note: Les métriques de function call nécessitent une génération complète\n",
        "    # qui est faite séparément dans l'évaluation détaillée\n",
        "    \n",
        "    # Calculer la perplexité à partir des logits\n",
        "    shift_logits = predictions[..., :-1, :]\n",
        "    shift_labels = labels[..., 1:]\n",
        "    \n",
        "    # Masquer les tokens de padding (-100)\n",
        "    mask = shift_labels != -100\n",
        "    \n",
        "    if mask.sum() > 0:\n",
        "        # Calculer la cross-entropy\n",
        "        from torch.nn import CrossEntropyLoss\n",
        "        loss_fct = CrossEntropyLoss(reduction='none')\n",
        "        \n",
        "        flat_logits = torch.tensor(shift_logits).view(-1, shift_logits.shape[-1])\n",
        "        flat_labels = torch.tensor(shift_labels).view(-1)\n",
        "        \n",
        "        losses = loss_fct(flat_logits, flat_labels)\n",
        "        masked_losses = losses * mask.view(-1).float()\n",
        "        \n",
        "        avg_loss = masked_losses.sum() / mask.sum()\n",
        "        perplexity = torch.exp(avg_loss).item()\n",
        "    else:\n",
        "        perplexity = float('inf')\n",
        "    \n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "    }\n",
        "\n",
        "print(\"✓ Métriques personnalisées définies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Configuration de l'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    TrainingArguments, \n",
        "    Trainer, \n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CONFIG[\"output_dir\"],\n",
        "    \n",
        "    # Epochs et batch\n",
        "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
        "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
        "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "    \n",
        "    # Optimisation\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],  # Cosine scheduler\n",
        "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
        "    weight_decay=CONFIG[\"weight_decay\"],\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # Logging\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=CONFIG[\"logging_steps\"],\n",
        "    report_to=[\"tensorboard\"],  # Activer TensorBoard\n",
        "    \n",
        "    # Évaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=CONFIG[\"eval_steps\"],\n",
        "    \n",
        "    # Sauvegarde\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=CONFIG[\"save_steps\"],\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    \n",
        "    # Performance\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Misc\n",
        "    remove_unused_columns=False,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, \n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStoppingCallback(\n",
        "        early_stopping_patience=CONFIG[\"early_stopping_patience\"],\n",
        "        early_stopping_threshold=CONFIG[\"early_stopping_threshold\"],\n",
        "    )\n",
        "]\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Configuration d'entraînement:\")\n",
        "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"  Batch: {CONFIG['batch_size']} × {CONFIG['gradient_accumulation_steps']} = {effective_batch}\")\n",
        "print(f\"  LR: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']})\")\n",
        "print(f\"  Early stopping: patience={CONFIG['early_stopping_patience']}\")\n",
        "print(f\"  TensorBoard: ./logs\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lancer TensorBoard (optionnel)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Début de l'entraînement...\")\n",
        "print(f\"  Train: {len(tokenized_dataset['train'])} exemples\")\n",
        "print(f\"  Val: {len(tokenized_dataset['validation'])} exemples\")\n",
        "print()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Entraînement terminé!\")\n",
        "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Steps: {train_result.global_step}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Évaluation finale\n",
        "print(\"Évaluation finale...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\nRésultats:\")\n",
        "print(f\"  Eval loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"  Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Évaluation détaillée des Function Calls\n",
        "\n",
        "Test de la qualité des prédictions sur des exemples spécifiques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_response(query: str, max_tokens: int = 150):\n",
        "    \"\"\"Génère une réponse du modèle.\"\"\"\n",
        "    text = f\"<start_of_turn>user\\n{query}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    \n",
        "    if \"<start_of_turn>model\" in response:\n",
        "        response = response.split(\"<start_of_turn>model\")[-1]\n",
        "    if \"<end_of_turn>\" in response:\n",
        "        response = response.split(\"<end_of_turn>\")[0]\n",
        "    \n",
        "    return response.strip()\n",
        "\n",
        "def evaluate_function_calls(test_cases):\n",
        "    \"\"\"Évalue la précision des function calls.\"\"\"\n",
        "    results = {\n",
        "        \"total\": 0,\n",
        "        \"correct_function\": 0,\n",
        "        \"correct_domain\": 0,\n",
        "        \"has_entity\": 0,\n",
        "        \"details\": []\n",
        "    }\n",
        "    \n",
        "    for test in test_cases:\n",
        "        query = test[\"query\"]\n",
        "        expected_func = test.get(\"expected_function\")\n",
        "        expected_domain = test.get(\"expected_domain\")\n",
        "        \n",
        "        response = generate_response(query)\n",
        "        func_name, entity_id = extract_function_call(response)\n",
        "        \n",
        "        results[\"total\"] += 1\n",
        "        \n",
        "        # Vérifier la fonction\n",
        "        if expected_func and func_name == expected_func:\n",
        "            results[\"correct_function\"] += 1\n",
        "        \n",
        "        # Vérifier le domaine (pour get_entities)\n",
        "        if expected_domain and func_name == \"get_entities\":\n",
        "            if expected_domain in response:\n",
        "                results[\"correct_domain\"] += 1\n",
        "        \n",
        "        # Vérifier si entity_id présent\n",
        "        if entity_id:\n",
        "            results[\"has_entity\"] += 1\n",
        "        \n",
        "        results[\"details\"].append({\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"function\": func_name,\n",
        "            \"entity\": entity_id,\n",
        "            \"expected\": expected_func,\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Définir les cas de test\n",
        "test_cases = [\n",
        "    {\"query\": \"Allume la lumière du salon\", \"expected_function\": \"get_entities\", \"expected_domain\": \"light\"},\n",
        "    {\"query\": \"Éteins la lumière de la cuisine\", \"expected_function\": \"get_entities\", \"expected_domain\": \"light\"},\n",
        "    {\"query\": \"Mets le chauffage à 21 degrés\", \"expected_function\": \"get_entities\", \"expected_domain\": \"climate\"},\n",
        "    {\"query\": \"Ferme les volets du salon\", \"expected_function\": \"get_entities\", \"expected_domain\": \"cover\"},\n",
        "    {\"query\": \"Active la scène cinéma\", \"expected_function\": \"get_entities\", \"expected_domain\": \"scene\"},\n",
        "    {\"query\": \"Verrouille la porte d'entrée\", \"expected_function\": \"get_entities\", \"expected_domain\": \"lock\"},\n",
        "    {\"query\": \"Allume le ventilateur\", \"expected_function\": \"get_entities\", \"expected_domain\": \"fan\"},\n",
        "    {\"query\": \"Éteins tout\", \"expected_function\": \"get_entities\", \"expected_domain\": \"light\"},\n",
        "]\n",
        "\n",
        "print(\"Évaluation des function calls...\\n\")\n",
        "results = evaluate_function_calls(test_cases)\n",
        "\n",
        "print(f\"Résultats ({results['total']} tests):\")\n",
        "print(f\"  Fonction correcte: {results['correct_function']}/{results['total']} ({results['correct_function']/results['total']*100:.1f}%)\")\n",
        "print(f\"  Domaine correct: {results['correct_domain']}/{results['total']} ({results['correct_domain']/results['total']*100:.1f}%)\")\n",
        "print(f\"  Avec entity_id: {results['has_entity']}/{results['total']}\")\n",
        "\n",
        "print(\"\\nDétails:\")\n",
        "for detail in results[\"details\"]:\n",
        "    status = \"✓\" if detail[\"function\"] == detail[\"expected\"] else \"✗\"\n",
        "    print(f\"  {status} {detail['query'][:40]}...\")\n",
        "    print(f\"      → {detail['function']}({detail['entity'] or '...'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_path = f\"{CONFIG['output_dir']}/final\"\n",
        "trainer.save_model(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "\n",
        "# Sauvegarder les métriques\n",
        "import json\n",
        "metrics = {\n",
        "    \"train_loss\": train_result.training_loss,\n",
        "    \"eval_loss\": eval_results['eval_loss'],\n",
        "    \"perplexity\": float(np.exp(eval_results['eval_loss'])),\n",
        "    \"config\": CONFIG,\n",
        "    \"function_call_accuracy\": results['correct_function'] / results['total'] if results['total'] > 0 else 0,\n",
        "}\n",
        "\n",
        "with open(f\"{final_path}/training_metrics.json\", 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"✓ Modèle sauvegardé: {final_path}\")\n",
        "print(f\"✓ Métriques sauvegardées: {final_path}/training_metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"functiongemma-ha\", 'zip', final_path)\n",
        "files.download(\"functiongemma-ha.zip\")\n",
        "print(\"✓ Téléchargement du modèle...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test du modèle\n",
        "\n",
        "**IMPORTANT:** Le format de test doit correspondre EXACTEMENT au format d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(query: str):\n",
        "    \"\"\"Test le modèle - format IDENTIQUE à l'entraînement.\"\"\"\n",
        "    response = generate_response(query)\n",
        "    return response\n",
        "\n",
        "# Tests avec des requêtes similaires au dataset\n",
        "test_queries = [\n",
        "    \"Allume la lumière du salon\",\n",
        "    \"Éteins la lumière de la cuisine\",\n",
        "    \"Mets le chauffage à 21 degrés\",\n",
        "    \"Active le thermostat\",\n",
        "    \"Active la scène cinéma\",\n",
        "    \"Ferme les volets\",\n",
        "    # Avec typos\n",
        "    \"alume la lumiere du salon\",\n",
        "    \"etein la cuisine\",\n",
        "]\n",
        "\n",
        "print(\"Tests du modèle fine-tuné:\\n\")\n",
        "for query in test_queries:\n",
        "    print(f\"User: {query}\")\n",
        "    response = test_model(query)\n",
        "    print(f\"Model: {response}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Test multi-turn complet\n",
        "\n",
        "Simuler le flow complet: query → get_entities → tool response → action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_multiturn(query: str, fake_entities: str):\n",
        "    \"\"\"Test le flow multi-turn complet.\"\"\"\n",
        "    \n",
        "    # Étape 1: Requête utilisateur → get_entities\n",
        "    text1 = f\"<start_of_turn>user\\n{query}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    inputs1 = tokenizer(text1, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out1 = model.generate(**inputs1, max_new_tokens=80, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    \n",
        "    resp1 = tokenizer.decode(out1[0], skip_special_tokens=False)\n",
        "    if \"<start_of_turn>model\" in resp1:\n",
        "        resp1 = resp1.split(\"<start_of_turn>model\")[-1]\n",
        "    if \"<end_of_turn>\" in resp1:\n",
        "        resp1 = resp1.split(\"<end_of_turn>\")[0]\n",
        "    resp1 = resp1.strip()\n",
        "    \n",
        "    print(f\"User: {query}\")\n",
        "    print(f\"Model (step 1): {resp1}\")\n",
        "    \n",
        "    # Étape 2: Injecter la réponse tool → action finale\n",
        "    text2 = (\n",
        "        f\"<start_of_turn>user\\n{query}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n{resp1}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>tool\\n{fake_entities}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "    )\n",
        "    inputs2 = tokenizer(text2, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out2 = model.generate(**inputs2, max_new_tokens=80, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    \n",
        "    resp2 = tokenizer.decode(out2[0], skip_special_tokens=False)\n",
        "    if \"<start_of_turn>model\" in resp2:\n",
        "        resp2 = resp2.split(\"<start_of_turn>model\")[-1]\n",
        "    if \"<end_of_turn>\" in resp2:\n",
        "        resp2 = resp2.split(\"<end_of_turn>\")[0]\n",
        "    resp2 = resp2.strip()\n",
        "    \n",
        "    print(f\"Tool: {fake_entities[:80]}...\")\n",
        "    print(f\"Model (step 2): {resp2}\")\n",
        "    print()\n",
        "\n",
        "# Tests\n",
        "print(\"=\" * 50)\n",
        "print(\"Test Multi-Turn\")\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "test_multiturn(\n",
        "    \"Allume la lumière du salon\",\n",
        "    \"Entités light disponibles: light.salon, light.cuisine, light.chambre\"\n",
        ")\n",
        "\n",
        "test_multiturn(\n",
        "    \"Mets le chauffage à 22 degrés\",\n",
        "    \"Entités climate disponibles: climate.thermostat_salon, climate.thermostat_bureau\"\n",
        ")\n",
        "\n",
        "test_multiturn(\n",
        "    \"Ferme les volets de la chambre\",\n",
        "    \"Entités cover disponibles: cover.volets_salon, cover.volets_chambre, cover.volets_cuisine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Résumé et prochaines étapes\n",
        "\n",
        "### Métriques finales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"RÉSUMÉ DE L'ENTRAÎNEMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModèle: {CONFIG['model_name']}\")\n",
        "print(f\"LoRA rank: {CONFIG['lora_r']} (alpha: {CONFIG['lora_alpha']})\")\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Train: {len(tokenized_dataset['train'])} exemples\")\n",
        "print(f\"  Validation: {len(tokenized_dataset['validation'])} exemples\")\n",
        "print(f\"\\nEntraînement:\")\n",
        "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']} ({CONFIG['lr_scheduler_type']})\")\n",
        "print(f\"  Batch size: {effective_batch} (effective)\")\n",
        "print(f\"\\nRésultats:\")\n",
        "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Eval loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"  Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
        "print(f\"  Function call accuracy: {results['correct_function']/results['total']*100:.1f}%\")\n",
        "print(f\"\\nFichiers:\")\n",
        "print(f\"  Modèle: {final_path}/\")\n",
        "print(f\"  Métriques: {final_path}/training_metrics.json\")\n",
        "print(f\"  Logs: ./logs/\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
